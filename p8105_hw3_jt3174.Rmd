---
title: "p8105_hw3_jt3174"
author: "Jingyi"
date: "10/4/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document

# ensure reproductivity
set.seed(1)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)

# load the dataset library
library(p8105.datasets)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```


## **Problem 1**

```{r problem 1}

# find the specific data file
data("instacart")

# Part: Aisle

aisle_data = instacart %>% 
  # group by aisle name
  group_by(aisle) %>% 
  # count the number of order for each distinct aisle
  distinct() %>% 
  summarize(n_aisle = n()) %>% 
  # sort this list descendently, so the first row is the most ordered aisle
  arrange(desc(n_aisle))

aisle_data

# Part: Plot for Aisle

# make a good dataframe for this plot first
aisle_plot = aisle_data %>% 
  # limiting this to aisles with more than 10000 items ordered
  filter(n_aisle > 10000) %>% 
  # change the order of the datafram
  select(n_aisle, everything()) %>% 
  # change the order of factor aisle according to n_aisle
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle)) 

aisle_plot

# make a column chart for this good dataframe
# use aisle_plot dataframe, x-axis is alsle and y-axis is the number of orders
aisle_col_chart = ggplot(aisle_plot, aes(x = aisle, y = n_aisle))+
  # fill the color according to its aisle
  geom_col(aes(fill = aisle)) +
  labs(
    # give title, x-axis name, y-axis name and caption
    title = "Sales of Each Aisle",
    x = "Aisle",
    y = "Number of Order",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017” Website"
  ) +
  # don't show the original x-axis labels for readability
  scale_x_discrete(labels = NULL)

aisle_col_chart

# Part: Table for Most Popular Items

# create a good dataframe for this table first
popular_item_table = instacart %>% 
  # group by aisle and product name
  group_by(aisle, product_name) %>% 
  # keep only the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  filter(aisle == "baking ingredients" | aisle == "dog food care" |  aisle =="packaged vegetables fruits") %>% 
  # count the number of order
  summarize(sales = n()) %>% 
  # keep only the most popular three product in each aisle
  filter(min_rank(desc(sales)) < 4) %>% 
  # arrange according to sales for each group
  arrange(desc(sales)) %>% 
  # knit a table
  knitr::kable(digits = 1)
  
popular_item_table

# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

# Part: Table for mean hour

# Create a good dataframe for this table
mean_hour_table = instacart %>% 
  # filter out Pink Lady Apples and Coffee Ice Cream
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  # select only product_name, order_dow, order_hour_of_day variables
  select(product_name, order_dow, order_hour_of_day) %>% 
  # group by product_name and order_dow
  group_by(product_name, order_dow) %>% 
  # find mean of order_hour_of_day
  summarize(
    hour_of_day_mean = mean(order_hour_of_day)) %>% 
  # make a 2x7 table
  pivot_wider(
    names_from = order_dow,
    values_from = hour_of_day_mean
  ) %>% 
  # rename the day of week from numbers to corresponding English vocabularies
  rename(Product = "product_name", Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
   # give the format of table for knit
  knitr::kable(digit = 1)

mean_hour_table

```

Short description of the dataset:

This dataset called instacart is a dataset contains information about online grocery orders from more than 200,000 Instacart users and there are `r count(instacart)` observations of 131209 customers in the year of 2017, where each row in the dataset is a product from an order. There is a single order per user in this dataset.

There are 15 variables in this dataset:

* order_id: order identifier
* product_id: product identifier
* add_to_cart_order: order in which each product was added to cart
* reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
* user_id: customer identifier
* eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
* order_number: the order sequence number for this user (1=first, n=nth)
* order_dow: the day of the week on which the order was placed
* order_hour_of_day: the hour of the day on which the order was placed
* days_since_prior_order: days since the last order, capped at 30, NA if order_number=1
* product_name: name of the product
* aisle_id: aisle identifier
* department_id: department identifier
* aisle: the name of the aisle
* department: the name of the department

Some of the key variables are reordered (this tells you whether the item was repurchased or not), and  days_since_prior_order (this tells you how long it takes to reorder an item).

As I went through the process of creating the plots above, I think some useful illustrations of observaitions can be the 2 x 7 table we created. We can find the most popular product within each aisle, and maybe use this data to prepare the inventory and keep on track of how many inventory the store should have on hand to sale. And if we combine this with time of order or date of order, we can maybe find a pattern for certain products and their best selling dates, (for example, turkey and thanksgiving day) to properly prepare our inventory in case that some certain goods would be out-of-stock on certain period of time and having excessive inventory for other times. 

There are `r count(aisle_data)` aisles, and the most popular aisle is fresh vegetables.




## **Problem 2**
This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.

```{r problem 2}

# find the specific data file
data("brfss_smart2010")

# data cleaning

# create a dataframe
brfss_data = brfss_smart2010 %>% 
  # clean the names
  janitor::clean_names() %>% 
  # rename some variables 
  rename("state" = "locationabbr", "response_id" = "respid") %>% 
  # separate locationdesc into 2 new variables: the state name and the county name (aka location)
  separate(locationdesc, into = c("state_1", "county"), sep = 5) %>% 
  # remove the redundant state variable
  select(-state_1) %>% 
  # keep only "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter(response == "Poor" | response == "Fair" | response == "Good" | response == "Very good" | response == "Excellent") %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
  
brfss_data

```

```{r problem 2: 7 or more location}

# create a good dataframe for this part
# for the year of 2002
states_2002 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2002") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2002

# for the year of 2010
states_2010 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2010") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2010

```

For the year of 2002, the states were observed at 7 or more locations are Connecticut, Florida, Massachusetts, North Carolina, New Jersey, Pennsylvania.
For the year of 2010, the states were observed at 7 or more locations are California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pannsylvania, South Carolina, Texas, Washington.

```{r construct dataset}

# make a good datadrame for the plot 
excellent_data = brfss_data %>% 
  # keep only excellent responses
  filter(response == "Excellent") %>% 
  # select only year, state, data_value, location
  select(year, state, county, data_value) %>% 
  # group by year and state
  group_by(year, state) %>% 
  # find the mean for data_value for each year and state
  summarize(mean_data = mean(data_value, na.rm = TRUE))

excellent_data

# make a Spaghetti Plot for Mean Data Value over Year within a State
# x-axis: year, y-axis = mean, assign color according to their states
excellent_plot = ggplot(excellent_data, aes(x = year, y = mean_data, color = state)) + 
  geom_line() +
  # add title, labels for x- and y-axis, caption
  labs(
    title = "Spaghetti Plot for Mean Data Value over Year within a State",
    x = "Year",
    y = " Mean Data Value across Locations within a State",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

excellent_plot

```

The spaghetti plot indicates that for excellent responses, the average data value for each state across its locations(counties) from 2002 to 2010 is relatively stable (which means there is no big change/jump through this period of time).

```{r two-panel plot}

# create a good dataset for making this plot
two_panel_plot = brfss_data %>% 
  # keep only the data of 2006
  filter(year == "2006" | year == "2010") %>% 
  # keep only the data of NY state
  filter(state == "NY") %>% 
  # group by responses
  group_by(response) %>% 
  # making a density plot for data_value and fill different color according to their different responses
  ggplot(aes(x = data_value, fill = response))+
  # set transparency
  geom_density(alpha = 0.5， adjust = 0.5) +
  # create a two panel plot for responses among locations in NY State
  facet_grid(year ~ response) +
  # add title, labels for x- and y- labels and caption
  labs(
    title = "Two Panel Plot for Responses among Locations in NY State",
    x = "Data Value",
    y = "Density",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

two_panel_plot

```

Within the state of NY, the distributions of each response given the same response is mostly similar between the year of 2006 and 2010. The Fair response differed a little, but it is not terribly different. 

## **Problem 3**

```{r load and tidy data}

accel_data = 
  # read accel data
  read_csv("./data/accel_data.csv") %>%
  # clean the names and have useful variable names
  janitor::clean_names() %>%
  # tidy data
  pivot_longer(
    # find all activity counts for each minute in a day
    activity_1:activity_1440,
    # put activity_* to a new variable minute_in_a_day
    names_to = "minute_in_a_day",
    # put values into coresponding cell in a new variable activity_count
    values_to = "activity_count",
    # get rid of the prefix activity_ for values in minute_in_a_day
    names_prefix = "activity_"
    ) %>% 
  mutate(
    # to include a weekday vs weekend variable
    # make a new variable takes the same values as day variable
    day_1 = day,
    # recode each values to a corresponding integer
    day_1 = recode(day, "Monday" = "1" ,"Tuesday" =  "2", "Wednesday" = "3", "Thursday" = "4", "Friday" = "5", "Saturday" = "6", "Sunday" = "7"),
    # include a weekday vs weekend variable
    weekday = case_when(
      # use case_when to assign weekdays and weekends to each different day values
      day_1 <= 5 ~ "weekday",
      day_1 > 5 ~ "weekend",
      TRUE    ~ ""
    ),
    # encode data with reasonable variable classes
    minute_in_a_day = as.numeric(minute_in_a_day)
    ) %>% 
  # remove the redundant variable
  select(-day_1)

accel_data

```

This dataser called accel_data uses five weeks of accelerometer data collected on a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF).

In this dataset, there are `r count(accel_data)` observations and 6 variables exist. 

The variables are:

* week: the number of the week of this man's data record
* day_id: the number of the day of this man's data record
* day: the day of week of this man's data record
* minute_in_a_day: the minute of a day of this man's data record
* activity_count: the activity counts for each minute of a 24-hour day starting at midnight for this man's data record
* weekday: to indicate whether it is a weekday or a weekend for this man's data record


```{r}

total_each_day = accel_data %>%
  # group by week and day of week, so we would have data for 35 days
  group_by(week, day) %>% 
  # find the total for each day
  summarize(sum_of_day = sum(activity_count)) %>% 
  # create a table for this
  knitr::kable(digits = 2)

total_each_day

# to identify a trend, we create a scatterplot connected with a line to check

trend_plot = accel_data %>%
  # group by week and day of week, so we would have data for 35 days
  group_by(week, day) %>% 
  # find the total for each day
  summarize(sum_of_day = sum(activity_count)) %>% 
  # ungroup week and day
  ungroup(week, day) %>% 
  # add a variable to be the index
  mutate(day_number = row_number())

trend_plot %>% 
  # create a scatterplot and let day_number to be x-axis and sum_of_day to be 
  ggplot(aes(x = day_number, y = sum_of_day)) +
  geom_point(aes(color = day_number), alpha = 0.5) +
  geom_line(aes(color = day_number)) +
  # add title, labels for x- and y- axis and caption
  labs(
    title = "Trend Plot for Sum of Day Value Across Days",
    x = "Days",
    y = "Total Activity Counts for Each Day",
    caption = "Data from the Advanced Cardiac Care Center of Columbia University Medical Center"
  ) +
  scale_y_continuous(
    # make the y-axis labels a bit prettier
    breaks = c(0, 200000, 400000, 600000, 800000),
    labels = c("0", "200000", "400000", "600000", "800000")
    )

```

There is no appearant trend over time for the activity counts in each day. The number just oscalate in the range from 1000 to 650000. 

```{r}

inspection_data = accel_data %>%
  # group by week and day to have 35 days
  group_by(week, day) %>% 
  # organize minutes to hours
  mutate(hour_in_a_day = ceiling(minute_in_a_day/60)) %>% 
  # group by week, day and each hour
  group_by(week, day, hour_in_a_day) %>% 
  # Find the total activity for each hour for each day
  summarize(sum_by_hour = sum(activity_count)) %>% 
  # plot with x = hour_in_a_day and y-axis is sum_by_hour
  ggplot(aes(x = hour_in_a_day, y = sum_by_hour)) +
  # color by day
  geom_point(aes(color = day)) +
  # give labels and captions for the plot
  labs(
    title = "Plot for 24-hour Activity Time Courses for Each Day",
    x = "Hour in a Day",
    y = "Total Activity Counts for Each Hour",
    caption = "Data from the Advanced Cardiac Care Center of Columbia University Medical Center") +
    scale_y_continuous(
    # make the y-axis labels a bit prettier
    breaks = c(0, 20000, 40000, 60000, 80000, 100000， 120000),
    labels = c("0", "20000", "40000", "60000", "80000", "100000", "120000")
  ) +
    scale_x_continuous(
    # make the x-axis labels a bit prettier
    breaks = c(0, 4, 8, 12, 16, 20, 24),
    labels = c("0", "4", "8", "12", "16", "20", "24"))
  
inspection_data

```

According to the plot that R generated, we can see that this man was extremly inactive during 0AM - 8AM everyday, and then pretty much stayed the same level of activeness for the rest of day after 8AM to 8PM. He moved around a lot at Friday nights and Sunday mornings relatively. Then his activity counts gradually decreased after 8PM.