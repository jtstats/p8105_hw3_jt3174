---
title: "p8105_hw3_jt3174"
author: "Jingyi"
date: "10/4/2019"
output: github_document
---

```{r set up, include = FALSE}
# knitr will run the chunk but not include the chunk in the final document

# ensure reproductivity
set.seed(1)

# load library
library(tidyverse)
library(viridis)
library(ggridges)
library(patchwork)

# load the dataset library
library(p8105.datasets)

knitr::opts_chunk$set(
  # display the code in the code truck above its results in the final document
  echo = TRUE,
  # do not display any warning messages generated by the code
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 90%
  fig.width = 8,
  fig.height = 6, 
  out.width = "90%"
)

# setting a global options for continuous data color family and a different format to set discrete data to have a color family
options(
  ggplot2.countinuous.colour = "viridis",
  ggplot2.countinuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# have a minimal theme and legends at the bottom
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```


## **Problem 1**

```{r problem 1}

# find the specific data file
data("instacart")

# Part: Aisle

aisle_data = instacart %>% 
  # group by aisle name
  group_by(aisle) %>% 
  # count the number of order for each distinct aisle
  distinct() %>% 
  summarize(n_aisle = n()) %>% 
  # sort this list descendently, so the first row is the most ordered aisle
  arrange(desc(n_aisle))

aisle_data

# Part: Plot for Aisle

# make a good dataframe for this plot first
aisle_plot = aisle_data %>% 
  # limiting this to aisles with more than 10000 items ordered
  filter(n_aisle > 10000) %>% 
  # change the order of the datafram
  select(n_aisle, everything()) %>% 
  # change the order of factor aisle according to n_aisle
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle))

aisle_plot

# make a column chart for this good dataframe
# use aisle_plot dataframe, x-axis is alsle and y-axis is the number of orders
aisle_col_chart = ggplot(aisle_plot, aes(x = aisle, y = n_aisle))+
  # fill the color according to its aisle
  geom_col(aes(fill = aisle)) +
  labs(
    # give title, x-axis name, y-axis name and caption
    title = "Sales of Each Aisle",
    x = "Aisle",
    y = "Number of Order",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017” Website"
  )

aisle_col_chart

# Part: Table for Most Popular Items

# create a good dataframe for this table first
popular_item_table = instacart %>% 
  # group by aisle and product name
  group_by(aisle, product_name) %>% 
  # keep only the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  filter(aisle == "baking ingredients" | aisle == "dog food care" |  aisle =="packaged vegetables fruits") %>% 
  # count the number of order
  summarize(sales = n()) %>% 
  # keep only the most popular three product in each aisle
  filter(min_rank(desc(sales)) < 4) %>% 
  # arrange according to sales for each group
  arrange(desc(sales)) %>% 
  # knit a table
  knitr::kable(digits = 1)
  
popular_item_table

# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

# Part: Table for mean hour

# Create a good dataframe for this table
mean_hour_table = instacart %>% 
  # filter out Pink Lady Apples and Coffee Ice Cream
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  # select only product_name, order_dow, order_hour_of_day variables
  select(product_name, order_dow, order_hour_of_day) %>% 
  # group by product_name and order_dow
  group_by(product_name, order_dow) %>% 
  # find mean of order_hour_of_day
  summarize(
    hour_of_day_mean = mean(order_hour_of_day)) %>% 
  # make a 2x7 table
  pivot_wider(
    names_from = order_dow,
    values_from = hour_of_day_mean
  ) %>% 
  # rename the day of week from numbers to corresponding English vocabularies
  rename(Product = "product_name", Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
   # give the format of table for knit
  knitr::kable(digit = 1)

mean_hour_table

```

Short description of the dataset:

This dataset called instacart is a dataset contains information about online grocery orders from more than 200,000 Instacart users and there are `r count(instacart)` observations of 131209 customers in the year of 2017, where each row in the dataset is a product from an order. There is a single order per user in this dataset.

There are 15 variables in this dataset:

* order_id: order identifier
* product_id: product identifier
* add_to_cart_order: order in which each product was added to cart
* reordered: 1 if this prodcut has been ordered by this user in the past, 0 otherwise
* user_id: customer identifier
* eval_set: which evaluation set this order belongs in (Note that the data for use in this class is exclusively from the “train” eval_set)
* order_number: the order sequence number for this user (1=first, n=nth)
* order_dow: the day of the week on which the order was placed
* order_hour_of_day: the hour of the day on which the order was placed
* days_since_prior_order: days since the last order, capped at 30, NA if order_number=1
* product_name: name of the product
* aisle_id: aisle identifier
* department_id: department identifier
* aisle: the name of the aisle
* department: the name of the department

Some of the key variables are reordered (this tells you whether the item was repurchased or not), and  days_since_prior_order (this tells you how long it takes to reorder an item).

There are `r count(aisle_data)` aisles, and the most popular aisle is fresh vegetables.




## **Problem 2**
This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.

```{r problem 2}

# find the specific data file
data("brfss_smart2010")

# data cleaning

# create a dataframe
brfss_data = brfss_smart2010 %>% 
  # clean the names
  janitor::clean_names() %>% 
  # rename some variables 
  rename("state" = "locationabbr", "response_id" = "respid") %>% 
  # separate locationdesc into 2 new variables: the state name and the county name (aka location)
  separate(locationdesc, into = c("state_1", "county"), sep = 5) %>% 
  # remove the redundant state variable
  select(-state_1) %>% 
  # keep only "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter(response == "Poor" | response == "Fair" | response == "Good" | response == "Very Good" | response == "Excellent") %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
  
brfss_data

```

```{r problem 2: 7 or more location}

# create a good dataframe for this part
# for the year of 2002
states_2002 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2002") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2002

# for the year of 2010
states_2010 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2010") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2010

```

For the year of 2002, the states were observed at 7 or more locations are Connecticut, Florida, Massachusetts, North Carolina, New Jersey, Pennsylvania.
For the year of 2010, the states were observed at 7 or more locations are California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pannsylvania, South Carolina, Texas, Washington.

```{r construct dataset}

# make a good datadrame for the plot 
excellent_data = brfss_data %>% 
  # keep only excellent responses
  filter(response == "Excellent") %>% 
  # select only year, state, data_value, location
  select(year, state, county, data_value) %>% 
  # group by year and state
  group_by(year, state) %>% 
  # find the mean for data_value for each year and state
  summarize(mean_data = mean(data_value, na.rm = TRUE))

excellent_data

excellent_plot = ggplot(excellent_data, aes(x = year, y = mean_data, color = state)) + 
  geom_line() +
  labs(
    title = "Spaghetti Plot for Mean Data Value over Year within a State",
    x = "Year",
    y = " Mean Data Value across Locations within a State",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

excellent_plot

```

The spaghetti plot indicates that for excellent responses, the average data value for each state across its locations(counties) from 2002 to 2010 is relatively stable (which means there is no big change/jump through this period of time).

```{r two-panel plot}

# create a good dataset for making this plot
two_panel_plot = brfss_data %>% 
  # keep only the data of 2006
  filter(year == "2006" | year == "2010") %>% 
  # keep only the data of NY state
  filter(state == "NY") %>% 
  # group by responses
  group_by(response) %>% 
  # making a density plot for data_value and fill different color according to their different responses
  ggplot(aes(x = data_value, fill = response))+
  # set transparency
  geom_density(alpha = 0.5) +
  # create a two panel plot for responses among locations in NY State
  facet_grid(year ~ response)+
  labs(
    title = "Two Panel Plot for Responses among Locations in NY State",
    x = "Data Value",
    y = "Density",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

two_panel_plot

```

Within the state of NY, the distributions of each response given the same response is mostly similar between the year of 2006 and 2010. The Fair response differed a little, but it is not terribly different. 

## **Problem 3**

```{r load and tidy data}

accel_data = 
  # read accel data
  read_csv("./data/accel_data.csv") %>%
  # clean the names and have useful variable names
  janitor::clean_names() %>%
  # tidy data
  pivot_longer(
    # find all activity counts for each minute in a day
    activity_1:activity_1440,
    # put activity_* to a new variable minute_in_a_day
    names_to = "minute_in_a_day",
    # put values into coresponding cell in a new variable activity_count
    values_to = "activity_count",
    # get rid of the prefix activity_ for values in minute_in_a_day
    names_prefix = "activity_"
    ) %>% 
  mutate(
    # to include a weekday vs weekend variable
    # make a new variable takes the same values as day variable
    day_1 = day,
    # recode each values to a corresponding integer
    day_1 = recode(day, "Monday" = "1" ,"Tuesday" =  "2", "Wednesday" = "3", "Thursday" = "4", "Friday" = "5", "Saturday" = "6", "Sunday" = "7"),
    # include a weekday vs weekend variable
    weekday = case_when(
      # use case_when to assign weekdays and weekends to each different day values
      day_1 <= 5 ~ "weekday",
      day_1 > 5 ~ "weekend",
      TRUE    ~ ""
    ),
    # encode data with reasonable variable classes
    minute_in_a_day = as.numeric(minute_in_a_day)
    ) %>% 
  # remove the redundant variable
  select(-day_1)

accel_data

```


Describe the resulting dataset (e.g. what variables exist, how many observations, etc).


Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.