p8105\_hw3\_jt3174
================
Jingyi
10/4/2019

## **Problem 1**

``` r
# find the specific data file
data("instacart")

# Part: Aisle

aisle_data = instacart %>% 
  # group by aisle name
  group_by(aisle) %>% 
  # count the number of order for each distinct aisle
  distinct() %>% 
  summarize(n_aisle = n()) %>% 
  # sort this list descendently, so the first row is the most ordered aisle
  arrange(desc(n_aisle))

aisle_data
```

    ## # A tibble: 134 x 2
    ##    aisle                         n_aisle
    ##    <chr>                           <int>
    ##  1 fresh vegetables               150609
    ##  2 fresh fruits                   150473
    ##  3 packaged vegetables fruits      78493
    ##  4 yogurt                          55240
    ##  5 packaged cheese                 41699
    ##  6 water seltzer sparkling water   36617
    ##  7 milk                            32644
    ##  8 chips pretzels                  31269
    ##  9 soy lactosefree                 26240
    ## 10 bread                           23635
    ## # ... with 124 more rows

``` r
# Part: Plot for Aisle

# make a good dataframe for this plot first
aisle_plot = aisle_data %>% 
  # limiting this to aisles with more than 10000 items ordered
  filter(n_aisle > 10000) %>% 
  # change the order of the datafram
  select(n_aisle, everything()) %>% 
  # change the order of factor aisle according to n_aisle
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle))

aisle_plot
```

    ## # A tibble: 39 x 2
    ##    n_aisle aisle                        
    ##      <int> <fct>                        
    ##  1  150609 fresh vegetables             
    ##  2  150473 fresh fruits                 
    ##  3   78493 packaged vegetables fruits   
    ##  4   55240 yogurt                       
    ##  5   41699 packaged cheese              
    ##  6   36617 water seltzer sparkling water
    ##  7   32644 milk                         
    ##  8   31269 chips pretzels               
    ##  9   26240 soy lactosefree              
    ## 10   23635 bread                        
    ## # ... with 29 more rows

``` r
# make a column chart for this good dataframe
# use aisle_plot dataframe, x-axis is alsle and y-axis is the number of orders
aisle_col_chart = ggplot(aisle_plot, aes(x = aisle, y = n_aisle))+
  # fill the color according to its aisle
  geom_col(aes(fill = aisle)) +
  labs(
    # give title, x-axis name, y-axis name and caption
    title = "Sales of Each Aisle",
    x = "Aisle",
    y = "Number of Order",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017” Website"
  )

aisle_col_chart
```

<img src="p8105_hw3_jt3174_files/figure-gfm/problem 1-1.png" width="90%" />

``` r
# Part: Table for Most Popular Items

# create a good dataframe for this table first
popular_item_table = instacart %>% 
  # group by aisle and product name
  group_by(aisle, product_name) %>% 
  # keep only the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  filter(aisle == "baking ingredients" | aisle == "dog food care" |  aisle =="packaged vegetables fruits") %>% 
  # count the number of order
  summarize(sales = n()) %>% 
  # keep only the most popular three product in each aisle
  filter(min_rank(desc(sales)) < 4) %>% 
  # arrange according to sales for each group
  arrange(desc(sales)) %>% 
  # knit a table
  knitr::kable(digits = 1)
  
popular_item_table
```

| aisle                      | product\_name                                 | sales |
| :------------------------- | :-------------------------------------------- | ----: |
| packaged vegetables fruits | Organic Baby Spinach                          |  9784 |
| packaged vegetables fruits | Organic Raspberries                           |  5546 |
| packaged vegetables fruits | Organic Blueberries                           |  4966 |
| baking ingredients         | Light Brown Sugar                             |   499 |
| baking ingredients         | Pure Baking Soda                              |   387 |
| baking ingredients         | Cane Sugar                                    |   336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |    28 |
| dog food care              | Small Dog Biscuits                            |    26 |

``` r
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

# Part: Table for mean hour

# Create a good dataframe for this table
mean_hour_table = instacart %>% 
  # filter out Pink Lady Apples and Coffee Ice Cream
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  # select only product_name, order_dow, order_hour_of_day variables
  select(product_name, order_dow, order_hour_of_day) %>% 
  # group by product_name and order_dow
  group_by(product_name, order_dow) %>% 
  # find mean of order_hour_of_day
  summarize(
    hour_of_day_mean = mean(order_hour_of_day)) %>% 
  # make a 2x7 table
  pivot_wider(
    names_from = order_dow,
    values_from = hour_of_day_mean
  ) %>% 
  # rename the day of week from numbers to corresponding English vocabularies
  rename(Product = "product_name", Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
   # give the format of table for knit
  knitr::kable(digit = 1)

mean_hour_table
```

| Product          | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
| :--------------- | -----: | -----: | ------: | --------: | -------: | -----: | -------: |
| Coffee Ice Cream |   13.8 |   14.3 |    15.4 |      15.3 |     15.2 |   12.3 |     13.8 |
| Pink Lady Apples |   13.4 |   11.4 |    11.7 |      14.2 |     11.6 |   12.8 |     11.9 |

Short description of the dataset:

This dataset called instacart is a dataset contains information about
online grocery orders from more than 200,000 Instacart users and there
are 1384617 observations of 131209 customers in the year of 2017, where
each row in the dataset is a product from an order. There is a single
order per user in this dataset.

There are 15 variables in this dataset:

  - order\_id: order identifier
  - product\_id: product identifier
  - add\_to\_cart\_order: order in which each product was added to cart
  - reordered: 1 if this prodcut has been ordered by this user in the
    past, 0 otherwise
  - user\_id: customer identifier
  - eval\_set: which evaluation set this order belongs in (Note that the
    data for use in this class is exclusively from the “train”
    eval\_set)
  - order\_number: the order sequence number for this user (1=first,
    n=nth)
  - order\_dow: the day of the week on which the order was placed
  - order\_hour\_of\_day: the hour of the day on which the order was
    placed
  - days\_since\_prior\_order: days since the last order, capped at 30,
    NA if order\_number=1
  - product\_name: name of the product
  - aisle\_id: aisle identifier
  - department\_id: department identifier
  - aisle: the name of the aisle
  - department: the name of the department

Some of the key variables are reordered (this tells you whether the item
was repurchased or not), and days\_since\_prior\_order (this tells you
how long it takes to reorder an item).

There are 134 aisles, and the most popular aisle is fresh vegetables.

## **Problem 2**

This problem uses the BRFSS data. DO NOT include this dataset in your
local data directory; instead, load the data from the p8105.datasets
package.

``` r
# find the specific data file
data("brfss_smart2010")

# data cleaning

# create a dataframe
brfss_data = brfss_smart2010 %>% 
  # clean the names
  janitor::clean_names() %>% 
  # rename some variables 
  rename("state" = "locationabbr", "response_id" = "respid") %>% 
  # separate locationdesc into 2 new variables: the state name and the county name (aka location)
  separate(locationdesc, into = c("state_1", "county"), sep = 5) %>% 
  # remove the redundant state variable
  select(-state_1) %>% 
  # keep only "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter(response == "Poor" | response == "Fair" | response == "Good" | response == "Very Good" | response == "Excellent") %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
  
brfss_data
```

    ## # A tibble: 8,500 x 23
    ##     year state county class topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 AL    Jeffe~ Heal~ Over~ How is ~ Excelle~          94       18.9
    ##  2  2010 AL    Jeffe~ Heal~ Over~ How is ~ Good             208       33.1
    ##  3  2010 AL    Jeffe~ Heal~ Over~ How is ~ Fair             107       12.5
    ##  4  2010 AL    Jeffe~ Heal~ Over~ How is ~ Poor              45        5.5
    ##  5  2010 AL    Mobil~ Heal~ Over~ How is ~ Excelle~          91       15.6
    ##  6  2010 AL    Mobil~ Heal~ Over~ How is ~ Good             224       31.2
    ##  7  2010 AL    Mobil~ Heal~ Over~ How is ~ Fair             120       15.5
    ##  8  2010 AL    Mobil~ Heal~ Over~ How is ~ Poor              66        6.4
    ##  9  2010 AL    Tusca~ Heal~ Over~ How is ~ Excelle~          58       20.8
    ## 10  2010 AL    Tusca~ Heal~ Over~ How is ~ Good             171       33.8
    ## # ... with 8,490 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, response_id <chr>, geo_location <chr>

``` r
# create a good dataframe for this part
# for the year of 2002
states_2002 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2002") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2002
```

    ## # A tibble: 6 x 2
    ##   state n_location
    ##   <chr>      <int>
    ## 1 CT             7
    ## 2 FL             7
    ## 3 MA             8
    ## 4 NC             7
    ## 5 NJ             8
    ## 6 PA            10

``` r
# for the year of 2010
states_2010 = brfss_data %>% 
  # keep only the data of 2002 year
  filter(year == "2010") %>% 
  # group by state
  group_by(state) %>% 
  # to ensure this is for distinct counties
  distinct(county) %>% 
  # make a state and number of distinct county dataframe
  summarize(n_location = n()) %>% 
  # filter out 7 or more locations
  filter(n_location >= 7)
  
states_2010
```

    ## # A tibble: 14 x 2
    ##    state n_location
    ##    <chr>      <int>
    ##  1 CA            12
    ##  2 CO             7
    ##  3 FL            41
    ##  4 MA             9
    ##  5 MD            12
    ##  6 NC            12
    ##  7 NE            10
    ##  8 NJ            19
    ##  9 NY             9
    ## 10 OH             8
    ## 11 PA             7
    ## 12 SC             7
    ## 13 TX            16
    ## 14 WA            10

For the year of 2002, the states were observed at 7 or more locations
are Connecticut, Florida, Massachusetts, North Carolina, New Jersey,
Pennsylvania. For the year of 2010, the states were observed at 7 or
more locations are California, Colorado, Florida, Massachusetts,
Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio,
Pannsylvania, South Carolina, Texas, Washington.

``` r
# make a good datadrame for the plot 
excellent_data = brfss_data %>% 
  # keep only excellent responses
  filter(response == "Excellent") %>% 
  # select only year, state, data_value, location
  select(year, state, county, data_value) %>% 
  # group by year and state
  group_by(year, state) %>% 
  # find the mean for data_value for each year and state
  summarize(mean_data = mean(data_value, na.rm = TRUE))

excellent_data
```

    ## # A tibble: 443 x 3
    ## # Groups:   year [9]
    ##     year state mean_data
    ##    <int> <chr>     <dbl>
    ##  1  2002 AK         27.9
    ##  2  2002 AL         18.5
    ##  3  2002 AR         24.1
    ##  4  2002 AZ         24.1
    ##  5  2002 CA         22.7
    ##  6  2002 CO         23.1
    ##  7  2002 CT         29.1
    ##  8  2002 DC         29.3
    ##  9  2002 DE         20.9
    ## 10  2002 FL         25.7
    ## # ... with 433 more rows

``` r
excellent_plot = ggplot(excellent_data, aes(x = year, y = mean_data, color = state)) + 
  geom_line() +
  labs(
    title = "Spaghetti Plot for Mean Data Value over Year within a State",
    x = "Year",
    y = " Mean Data Value across Locations within a State",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

excellent_plot
```

<img src="p8105_hw3_jt3174_files/figure-gfm/construct dataset-1.png" width="90%" />

The spaghetti plot indicates that for excellent responses, the average
data value for each state across its locations(counties) from 2002 to
2010 is relatively stable (which means there is no big change/jump
through this period of time).

``` r
# create a good dataset for making this plot
two_panel_plot = brfss_data %>% 
  # keep only the data of 2006
  filter(year == "2006" | year == "2010") %>% 
  # keep only the data of NY state
  filter(state == "NY") %>% 
  # group by responses
  group_by(response) %>% 
  # making a density plot for data_value and fill different color according to their different responses
  ggplot(aes(x = data_value, fill = response))+
  # set transparency
  geom_density(alpha = 0.5) +
  # create a two panel plot for responses among locations in NY State
  facet_grid(year ~ response)+
  labs(
    title = "Two Panel Plot for Responses among Locations in NY State",
    x = "Data Value",
    y = "Density",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010"
  )

two_panel_plot
```

<img src="p8105_hw3_jt3174_files/figure-gfm/two-panel plot-1.png" width="90%" />

Within the state of NY, the distributions of each response given the
same response is mostly similar between the year of 2006 and 2010. The
Fair response differed a little, but it is not terribly different.

## **Problem 3**

``` r
accel_data = 
  # read accel data
  read_csv("./data/accel_data.csv") %>%
  # clean the names and have useful variable names
  janitor::clean_names() %>%
  # tidy data
  pivot_longer(
    # find all activity counts for each minute in a day
    activity_1:activity_1440,
    # put activity_* to a new variable minute_in_a_day
    names_to = "minute_in_a_day",
    # put values into coresponding cell in a new variable activity_count
    values_to = "activity_count",
    # get rid of the prefix activity_ for values in minute_in_a_day
    names_prefix = "activity_"
    ) %>% 
  mutate(
    # to include a weekday vs weekend variable
    # make a new variable takes the same values as day variable
    day_1 = day,
    # recode each values to a corresponding integer
    day_1 = recode(day, "Monday" = "1" ,"Tuesday" =  "2", "Wednesday" = "3", "Thursday" = "4", "Friday" = "5", "Saturday" = "6", "Sunday" = "7"),
    # include a weekday vs weekend variable
    weekday = case_when(
      # use case_when to assign weekdays and weekends to each different day values
      day_1 <= 5 ~ "weekday",
      day_1 > 5 ~ "weekend",
      TRUE    ~ ""
    ),
    # encode data with reasonable variable classes
    minute_in_a_day = as.numeric(minute_in_a_day)
    ) %>% 
  # remove the redundant variable
  select(-day_1)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
accel_data
```

    ## # A tibble: 50,400 x 6
    ##     week day_id day    minute_in_a_day activity_count weekday
    ##    <dbl>  <dbl> <chr>            <dbl>          <dbl> <chr>  
    ##  1     1      1 Friday               1           88.4 weekday
    ##  2     1      1 Friday               2           82.2 weekday
    ##  3     1      1 Friday               3           64.4 weekday
    ##  4     1      1 Friday               4           70.0 weekday
    ##  5     1      1 Friday               5           75.0 weekday
    ##  6     1      1 Friday               6           66.3 weekday
    ##  7     1      1 Friday               7           53.8 weekday
    ##  8     1      1 Friday               8           47.8 weekday
    ##  9     1      1 Friday               9           55.5 weekday
    ## 10     1      1 Friday              10           43.0 weekday
    ## # ... with 50,390 more rows

This dataser called accel\_data uses five weeks of accelerometer data
collected on a 63 year-old male with BMI 25, who was admitted to the
Advanced Cardiac Care Center of Columbia University Medical Center and
diagnosed with congestive heart failure (CHF).

In this dataset, there are 50400 observations and 6 variables exist.

The variables are: \* week = the number of the week of this man’s data
record \* day\_id = the number of the day of this man’s data record \*
day = the day of week of this man’s data record \* minute\_in\_a\_day =
the minute of a day of this man’s data record \* activity\_count = the
activity counts for each minute of a 24-hour day starting at midnight
for this man’s data record \* weekday = to indicate whether it is a
weekday or a weekend for this man’s data record

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate accross minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent? Accelerometer data allows
the inspection activity over the course of the day. Make a single-panel
plot that shows the 24-hour activity time courses for each day and use
color to indicate day of the week. Describe in words any patterns or
conclusions you can make based on this graph.
