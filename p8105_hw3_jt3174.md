p8105\_hw3\_jt3174
================
Jingyi
10/4/2019

## **Problem 1**

``` r
# find the specific data file
data("instacart")

# Part: Aisle

aisle_data = instacart %>% 
  # group by aisle name
  group_by(aisle) %>% 
  # count the number of order for each distinct aisle
  distinct() %>% 
  summarize(n_aisle = n()) %>% 
  # sort this list descendently, so the first row is the most ordered aisle
  arrange(desc(n_aisle))

aisle_data
```

    ## # A tibble: 134 x 2
    ##    aisle                         n_aisle
    ##    <chr>                           <int>
    ##  1 fresh vegetables               150609
    ##  2 fresh fruits                   150473
    ##  3 packaged vegetables fruits      78493
    ##  4 yogurt                          55240
    ##  5 packaged cheese                 41699
    ##  6 water seltzer sparkling water   36617
    ##  7 milk                            32644
    ##  8 chips pretzels                  31269
    ##  9 soy lactosefree                 26240
    ## 10 bread                           23635
    ## # ... with 124 more rows

``` r
# Part: Plot for Aisle

# make a good dataframe for this plot first
aisle_plot = aisle_data %>% 
  # limiting this to aisles with more than 10000 items ordered
  filter(n_aisle > 10000) %>% 
  # change the order of the datafram
  select(n_aisle, everything()) %>% 
  # change the order of factor aisle according to n_aisle
  mutate(aisle = forcats::fct_reorder(aisle, n_aisle))

aisle_plot
```

    ## # A tibble: 39 x 2
    ##    n_aisle aisle                        
    ##      <int> <fct>                        
    ##  1  150609 fresh vegetables             
    ##  2  150473 fresh fruits                 
    ##  3   78493 packaged vegetables fruits   
    ##  4   55240 yogurt                       
    ##  5   41699 packaged cheese              
    ##  6   36617 water seltzer sparkling water
    ##  7   32644 milk                         
    ##  8   31269 chips pretzels               
    ##  9   26240 soy lactosefree              
    ## 10   23635 bread                        
    ## # ... with 29 more rows

``` r
# make a column chart for this good dataframe
# use aisle_plot dataframe, x-axis is alsle and y-axis is the number of orders
aisle_col_chart = ggplot(aisle_plot, aes(x = aisle, y = n_aisle))+
  # fill the color according to its aisle
  geom_col(aes(fill = aisle)) +
  labs(
    # give title, x-axis name, y-axis name and caption
    title = "Sales of Each Aisle",
    x = "Aisle",
    y = "Number of Order",
    caption = "Data from The Instacart Online Grocery Shopping Dataset 2017” Website"
  )

aisle_col_chart
```

<img src="p8105_hw3_jt3174_files/figure-gfm/problem 1-1.png" width="90%" />

``` r
# Part: Table for Most Popular Items

# create a good dataframe for this table first
popular_item_table = instacart %>% 
  # group by aisle and product name
  group_by(aisle, product_name) %>% 
  # keep only the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
  filter(aisle == "baking ingredients" | aisle == "dog food care" |  aisle =="packaged vegetables fruits") %>% 
  # count the number of order
  summarize(sales = n()) %>% 
  # keep only the most popular three product in each aisle
  filter(min_rank(desc(sales)) < 4) %>% 
  # arrange according to sales for each group
  arrange(desc(sales)) %>% 
  # knit a table
  knitr::kable(digits = 1)
  
popular_item_table
```

| aisle                      | product\_name                                 | sales |
| :------------------------- | :-------------------------------------------- | ----: |
| packaged vegetables fruits | Organic Baby Spinach                          |  9784 |
| packaged vegetables fruits | Organic Raspberries                           |  5546 |
| packaged vegetables fruits | Organic Blueberries                           |  4966 |
| baking ingredients         | Light Brown Sugar                             |   499 |
| baking ingredients         | Pure Baking Soda                              |   387 |
| baking ingredients         | Cane Sugar                                    |   336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |    28 |
| dog food care              | Small Dog Biscuits                            |    26 |

``` r
# Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)

# Part: Table for mean hour

# Create a good dataframe for this table
mean_hour_table = instacart %>% 
  # filter out Pink Lady Apples and Coffee Ice Cream
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  # select only product_name, order_dow, order_hour_of_day variables
  select(product_name, order_dow, order_hour_of_day) %>% 
  # group by product_name and order_dow
  group_by(product_name, order_dow) %>% 
  # find mean of order_hour_of_day
  summarize(
    hour_of_day_mean = mean(order_hour_of_day)) %>% 
  # make a 2x7 table
  pivot_wider(
    names_from = order_dow,
    values_from = hour_of_day_mean
  ) %>% 
  # rename the day of week from numbers to corresponding English vocabularies
  rename(Product = "product_name", Sunday = "0", Monday = "1", Tuesday = "2", Wednesday = "3", Thursday = "4", Friday = "5", Saturday = "6") %>% 
   # give the format of table for knit
  knitr::kable(digit = 1)

mean_hour_table
```

| Product          | Sunday | Monday | Tuesday | Wednesday | Thursday | Friday | Saturday |
| :--------------- | -----: | -----: | ------: | --------: | -------: | -----: | -------: |
| Coffee Ice Cream |   13.8 |   14.3 |    15.4 |      15.3 |     15.2 |   12.3 |     13.8 |
| Pink Lady Apples |   13.4 |   11.4 |    11.7 |      14.2 |     11.6 |   12.8 |     11.9 |

Short description of the dataset:

This dataset called instacart is a dataset contains information about
online grocery orders from more than 200,000 Instacart users and there
are 1384617 observations of 131209 customers in the year of 2017, where
each row in the dataset is a product from an order. There is a single
order per user in this dataset.

There are 15 variables in this dataset:

  - order\_id: order identifier
  - product\_id: product identifier
  - add\_to\_cart\_order: order in which each product was added to cart
  - reordered: 1 if this prodcut has been ordered by this user in the
    past, 0 otherwise
  - user\_id: customer identifier
  - eval\_set: which evaluation set this order belongs in (Note that the
    data for use in this class is exclusively from the “train”
    eval\_set)
  - order\_number: the order sequence number for this user (1=first,
    n=nth)
  - order\_dow: the day of the week on which the order was placed
  - order\_hour\_of\_day: the hour of the day on which the order was
    placed
  - days\_since\_prior\_order: days since the last order, capped at 30,
    NA if order\_number=1
  - product\_name: name of the product
  - aisle\_id: aisle identifier
  - department\_id: department identifier
  - aisle: the name of the aisle
  - department: the name of the department

Some of the key variables are reordered (this tells you whether the item
was repurchased or not), and days\_since\_prior\_order (this tells you
how long it takes to reorder an item).

There are 134 aisles, and the most popular aisle is fresh vegetables.

## **Problem 2**

This problem uses the BRFSS data. DO NOT include this dataset in your
local data directory; instead, load the data from the p8105.datasets
package.

``` r
# find the specific data file
data("brfss_smart2010")

# data cleaning

# create a dataframe
brfss_data = brfss_smart2010 %>% 
  # clean the names
  janitor::clean_names() %>% 
  # rename some variables 
  rename("state" = "locationabbr", "response_id" = "respid") %>% 
  # keep only "Overall Health" topic
  filter(topic == "Overall Health") %>% 
  # include only responses from "Excellent" to "Poor"
  filter(response == "Poor" | response == "Fair" | response == "Good" | response == "Very Good" | response == "Excellent") %>% 
  # organize responses as a factor taking levels ordered from “Poor” to “Excellent”
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
  
brfss_data
```

    ## # A tibble: 8,500 x 23
    ##     year state locationdesc class topic question response sample_size
    ##    <int> <chr> <chr>        <chr> <chr> <chr>    <fct>          <int>
    ##  1  2010 AL    AL - Jeffer~ Heal~ Over~ How is ~ Excelle~          94
    ##  2  2010 AL    AL - Jeffer~ Heal~ Over~ How is ~ Good             208
    ##  3  2010 AL    AL - Jeffer~ Heal~ Over~ How is ~ Fair             107
    ##  4  2010 AL    AL - Jeffer~ Heal~ Over~ How is ~ Poor              45
    ##  5  2010 AL    AL - Mobile~ Heal~ Over~ How is ~ Excelle~          91
    ##  6  2010 AL    AL - Mobile~ Heal~ Over~ How is ~ Good             224
    ##  7  2010 AL    AL - Mobile~ Heal~ Over~ How is ~ Fair             120
    ##  8  2010 AL    AL - Mobile~ Heal~ Over~ How is ~ Poor              66
    ##  9  2010 AL    AL - Tuscal~ Heal~ Over~ How is ~ Excelle~          58
    ## 10  2010 AL    AL - Tuscal~ Heal~ Over~ How is ~ Good             171
    ## # ... with 8,490 more rows, and 15 more variables: data_value <dbl>,
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, response_id <chr>, geo_location <chr>

``` r
# Part: 7 or more location

# create a good dataframe for this part

states_7 = brfss_data %>% 
  # separate locationdesc from the state name and the county name
  separate(locationdesc, into = c("state", "county"), sep = 5) %>% 
  select(-state)
  
states_7
```

    ## # A tibble: 8,500 x 22
    ##     year class county topic question response sample_size data_value
    ##    <int> <chr> <chr>  <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 Heal~ Jeffe~ Over~ How is ~ Excelle~          94       18.9
    ##  2  2010 Heal~ Jeffe~ Over~ How is ~ Good             208       33.1
    ##  3  2010 Heal~ Jeffe~ Over~ How is ~ Fair             107       12.5
    ##  4  2010 Heal~ Jeffe~ Over~ How is ~ Poor              45        5.5
    ##  5  2010 Heal~ Mobil~ Over~ How is ~ Excelle~          91       15.6
    ##  6  2010 Heal~ Mobil~ Over~ How is ~ Good             224       31.2
    ##  7  2010 Heal~ Mobil~ Over~ How is ~ Fair             120       15.5
    ##  8  2010 Heal~ Mobil~ Over~ How is ~ Poor              66        6.4
    ##  9  2010 Heal~ Tusca~ Over~ How is ~ Excelle~          58       20.8
    ## 10  2010 Heal~ Tusca~ Over~ How is ~ Good             171       33.8
    ## # ... with 8,490 more rows, and 14 more variables:
    ## #   confidence_limit_low <dbl>, confidence_limit_high <dbl>,
    ## #   display_order <int>, data_value_unit <chr>, data_value_type <chr>,
    ## #   data_value_footnote_symbol <chr>, data_value_footnote <chr>,
    ## #   data_source <chr>, class_id <chr>, topic_id <chr>, location_id <chr>,
    ## #   question_id <chr>, response_id <chr>, geo_location <chr>

First, do some data cleaning:

format the data to use appropriate variable names; focus on the “Overall
Health” topic include only responses from “Excellent” to “Poor” organize
responses as a factor taking levels ordered from “Poor” to “Excellent”
Using this dataset, do or answer the following (commenting on the
results of each):

In 2002, which states were observed at 7 or more locations? What about
in 2010? Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state (that is, make a plot showing a line for
each state across years – the geom\_line geometry and group aesthetic
will help). Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

\#\#**Problem 3** Accelerometers have become an appealing alternative to
self-report techniques for studying physical activity in observational
studies and clinical trials, largely because of their relative
objectivity. During observation periods, the devices measure “activity
counts” in a short period; one-minute intervals are common. Because
accelerometers can be worn comfortably and unobtrusively, they produce
around-the-clock observations.

This problem uses five weeks of accelerometer data collected on a 63
year-old male with BMI 25, who was admitted to the Advanced Cardiac Care
Center of Columbia University Medical Center and diagnosed with
congestive heart failure (CHF). The data can be downloaded here. In this
spreadsheet, variables activity.\* are the activity counts for each
minute of a 24-hour day starting at midnight.

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc). Traditional
analyses of accelerometer data focus on the total activity over the day.
Using your tidied dataset, aggregate accross minutes to create a total
activity variable for each day, and create a table showing these totals.
Are any trends apparent? Accelerometer data allows the inspection
activity over the course of the day. Make a single-panel plot that shows
the 24-hour activity time courses for each day and use color to indicate
day of the week. Describe in words any patterns or conclusions you can
make based on this graph.
